# Baby Emotion Detection using audio

The Baby Emotion Detection from Audio project is a machine learning-based system designed to analyze and classify the emotions expressed in audio recordings of babies. The system can classify emotions into categories such as discomfort, hunger, burping, tiredness, and belly pain. This project leverages various technologies including Django, TensorFlow, NumPy, and Matplotlib to create a user-friendly web application for efficient emotion detection.

## Features

- **Audio Emotion Classification:** The system can analyze audio recordings of babies and classify the emotions expressed within them into categories like discomfort, hunger, burping, tiredness, and belly pain.

- **Web Application:** The project includes a web application built using Django, providing users with an accessible and intuitive interface to upload and analyze audio recordings.

- **Machine Learning:** TensorFlow is used for implementing machine learning models to perform audio emotion classification, ensuring accurate and reliable results.

- **Version Control:** Check the `requirements.txt` file for version control of the project's dependencies, ensuring compatibility and reproducibility.

## Installation

To set up and run the Baby Emotion Detection from Audio project, follow these steps:

1. Clone the project repository:

   ```
   git clone https://github.com/MRBPatel/baby_emotion.git
   ```

2. Navigate to the project directory:

   ```
   cd baby-emotion
   ```

3. Install the required dependencies using `pip` and the `requirements.txt` file:

   ```
   pip install -r requirements.txt
   ```

4. Run the Django development server:

   ```
   python manage.py runserver
   ```

5. Access the web application in your browser by visiting `http://localhost:8000`.

## Usage

1. Open the web application in your browser.

2. Upload an audio recording of a baby expressing emotion.

3. Submit the recording for analysis.

4. The system will process the audio and classify the baby's emotion into one of the predefined categories.

5. View the results on the web interface, which may include visualizations generated using Matplotlib for a better understanding of the analysis.

## Snipets
![login_page](https://github.com/MRBPatel/human_activity_recogniser/assets/69763309/c27f2aee-1beb-451d-8b76-f812238afa0b)
![home_page](https://github.com/MRBPatel/human_activity_recogniser/assets/69763309/2a30adda-7c58-4d8f-a8a5-3389c246d238)
![output_3](https://github.com/MRBPatel/human_activity_recogniser/assets/69763309/11e99bb3-b8e0-4083-b49b-8f124bad1862)
![output_4](https://github.com/MRBPatel/human_activity_recogniser/assets/69763309/e1a1efa5-3c08-462c-b24d-7af9bbf146a9)
![output_5](https://github.com/MRBPatel/human_activity_recogniser/assets/69763309/e1fb1f18-7a61-488d-8115-9598098081dd)
![output_2](https://github.com/MRBPatel/human_activity_recogniser/assets/69763309/bb5f824c-fbc7-43ac-9cd7-7454bb617344)

## Contributing

Contributions to this project are welcome! If you have ideas for improvements, additional features, or bug fixes, please feel free to open an issue or submit a pull request.
